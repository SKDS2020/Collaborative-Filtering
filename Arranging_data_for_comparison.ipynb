{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arranging the data - Create an utility matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sortedcontainers\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame with cities that belonging to all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_DataFrame_with_filt_cities(data,all_similarities_cities_lst_str):\n",
    "\n",
    "    def from_iterable_unique(iterables): # I edited the function chain.from_iterable\n",
    "        for it in iterables.str.split(', '):\n",
    "            c1=Counter(it)\n",
    "            set_it= set(it)\n",
    "            for _ in set_it:\n",
    "                yield c1.popitem()\n",
    "\n",
    "    def unique_searches_counter(iterables): # I edited the function chain.from_iterable\n",
    "        for it in iterables.str.split(', '):\n",
    "            c1=Counter(it)         \n",
    "            set_it= set(it)\n",
    "            cnt=0 #count the searces in each session (last->first)\n",
    "            for _ in set_it:\n",
    "                c1.popitem()\n",
    "                cnt+=1\n",
    "                yield cnt \n",
    "\n",
    "    # calculate the amount of different cities in each session\n",
    "    # data2=data.copy()\n",
    "    # data2['cityCodes'].str.split(', ').apply(sortedcontainers.SortedSet).apply(lambda x: rem_fw(x, all_similarities_cities))\n",
    "    data['cityCodes_filt']=data.apply(lambda x: [x for x in list(x['cityCodes'].split(', ')) if x in all_similarities_cities_lst_str], axis=1)\n",
    "    data['cityCodes_filt2']=data.apply(lambda x: list(dict.fromkeys([x for x in list(x['cityCodes'].split(', ')) if x in all_similarities_cities_lst_str])), axis=1)\n",
    "    data['n_cityCodes_filt2']=data.apply(lambda x: len(x['cityCodes_filt2']), axis=1)\n",
    "\n",
    "    lens = data['cityCodes'].str.split(', ').apply(sortedcontainers.SortedSet).map(len)\n",
    "\n",
    "    # create new dataframe, repeating or chaining as appropriate\n",
    "    res = pd.DataFrame({'userId': np.repeat(data['userId'], lens),\n",
    "                        'sessionId': np.repeat(data['sessionId'], lens),\n",
    "                        'unique_cityCodes': np.repeat(data['cityCodes'], lens),\n",
    "                        'unique_cityCodes_filt': np.repeat(data['cityCodes_filt'], lens),\n",
    "                        'unique_cityCodes_filt2': np.repeat(data['cityCodes_filt2'], lens),\n",
    "                        'nunique_cityCodes_filt2': np.repeat(data['n_cityCodes_filt2'], lens),\n",
    "                        'city_searches': from_iterable_unique(data['cityCodes']),\n",
    "                        'search_cnt': unique_searches_counter(data['cityCodes']), #From the end to the beginning of the session\n",
    "                        'n_unique_cities': lens}) # Num of different cities in  each session\n",
    "\n",
    "    res['city'], res['searches'] = zip(*res.city_searches)\n",
    "    #res[res['unique_cityCodes_filt'].str.split(', ').apply(sortedcontainers.SortedSet).apply(lambda x: rem_fw(x, all_similarities_cities))]\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_DataFrame(data):\n",
    "\n",
    "    def from_iterable_unique(iterables): # I edited the function chain.from_iterable\n",
    "        for it in iterables.str.split(', '):\n",
    "            c1=Counter(it)\n",
    "            set_it= set(it)\n",
    "            for _ in set_it:\n",
    "                yield c1.popitem()\n",
    "\n",
    "    def unique_searches_counter(iterables): # I edited the function chain.from_iterable\n",
    "        for it in iterables.str.split(', '):\n",
    "            c1=Counter(it)         \n",
    "            set_it= set(it)\n",
    "            cnt=0 #count the searces in each session (last->first)\n",
    "            for _ in set_it:\n",
    "                c1.popitem()\n",
    "                cnt+=1\n",
    "                yield cnt \n",
    "\n",
    "    # calculate the amount of different cities in each session\n",
    "    lens = data['cityCodes'].str.split(', ').apply(set).map(len)\n",
    "    # create new dataframe, repeating or chaining as appropriate\n",
    "    res = pd.DataFrame({'userId': np.repeat(data['userId'], lens),\n",
    "                        'sessionId': np.repeat(data['sessionId'], lens),\n",
    "                        'unique_cityCodes': np.repeat(data['cityCodes'], lens),\n",
    "                        'city_searches': from_iterable_unique(data['cityCodes']),\n",
    "                        'search_cnt': unique_searches_counter(data['cityCodes']), #From the end to the beginning of the session\n",
    "                        'n_unique_cities': lens}) # Num of different cities in  each session\n",
    "\n",
    "    res['city'], res['searches'] = zip(*res.city_searches)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter sessions that have at least N searches of different cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_sessions_N_dif_searches(data, required_searches):\n",
    "    return data.loc[(data['nunique_cityCodes_filt2'] >= required_searches)]\n",
    "\n",
    "def sessions_without_last(data):\n",
    "    return data.loc[(data['search_cnt'] > 1)]\n",
    "    \n",
    "def sessions_with_only_last(data):\n",
    "    return data.loc[(data['search_cnt'] == 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transforming_data (res):\n",
    "    utility_matrix= res.pivot_table(index='sessionId',columns = 'city', values='searches', aggfunc=np.sum, fill_value=0) #it also merges the sessionId\n",
    "    return utility_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unknown sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unknown_sessions (utility_matrix , unknown_sessions_list=[] ):\n",
    "    utility_matrix=utility_matrix.drop(unknown_sessions_list, axis=0, errors='ignore') # drop the sessions in the list if they exist\n",
    "    return utility_matrix\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "821203d0b7be942b18cbbd219e05ff4af57f12afa49d20e52708bbb6ea37606d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('final_prj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
